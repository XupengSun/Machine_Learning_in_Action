# 基于K-NearestNeighbor算法对高斯分布点分类

# 问题描述

A类点以（0，0）为中心、（1，0；0，1）为协方差矩阵的二维高斯分布；

B类点以（1，2）为中心、（1，0；0，2）为协方差矩阵的二维高斯分布；

随机生成300个A类点，200个B类点，并用k-最近邻的方法进行分类(可视化呈现)。

# 算法与原理

## k近邻法

所谓邻近样本，就是离它最近的k个样本，通过计算其与所有已知样本的距离来确定。kNN一般使用的是欧氏距离，即两点间的空间距离。两个n维向量A和B间的欧氏距离为：

![image-20210129181702920](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129181702920.png)

所以kNN分类的初始状态就是一些已知类别的样本（全部特征向量表示），对于一个未知类别的新样本，我们找到k个距离最小的邻居样本，看这k个邻居属于哪个类的多，我们就认为新样本也属于那个类。

一般过程：

1）计算已训练集中样本与待测样本之间的距离；

2）按距离排序；

3）选取与当前样本距离最小的k个邻居样本；

4）确定此k个样本中各个类别的频率；

5）频率最高的类别作为该样本的预测分类。

# 具体过程

## 1.  数据生成

根据题目要求用numpy生成500个点，将位置坐标放入dataSet数组中,前三百个为A类点,后三百个为B类点。再生成一个label数组，包含300个1(用来表示A类)和200个2(用来表示B类).

画图展示生成的效果:

![image-20210129181810262](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129181810262.png)



## 2.  编写KNN分类函数

通过numpy的广播机制可以轻松的计算处待分类点和数据集中已知点的欧式距离,将距离存入数组中,并对其进行升序排列,就可以找出K个最近邻的点的标签,然后根据标签占比返回判别的种类.

## 3.  划分训练集和数据集查找最优K值

按9:1的比例划分训练集和测试集,每次用permutation将数据集的顺序打乱,可以保证不会出现过拟合.在range(1,350)之间每隔5个不唱取一个k值,对该k值进行十次分类取均值判断在该k值下的分类正确率,获得对该随机高斯分布的最优K取值.结果如下(每次运行后生成的高斯分布有差异故而每次的k值也各不相同)

![image-20210129181903040](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129181903040.png)

![image-20210129181908606](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129181908606.png)

## 4.  根据最优K值绘制可视化分类图

在高斯点的生成范围内均匀的取200个空间点,对他们根据上一步得到的最优k值同高斯分布点进行k近邻分类,根据分类结果绘制分类结果

## 5.  分类结果

最优k值下的分类效果图:

![image-20210129181942294](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129181942294.png)

上图为根据程序求得的最佳K值下的分类效果图,为了做对比下面也绘制了其他k取值下的分类图.

K = 5

![image-20210129181954756](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129181954756.png)

K = 350 

![image-20210129182006481](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129182006481.png)

K = 450 

![image-20210129182018965](https://gitee.com/sun-roc/picture/raw/master/img/image-20210129182018965.png)

可以看到的是最优k值下的分类效果明显优于较大或较小取值下的分类效果图.